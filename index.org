#+TITLE: Data Compression Techniques
#+SUBTITLE: An overview of some data compression techniques
#+AUTHOR: Ashiwn Godbole
#+LATEX_HEADER: \usepackage[margin=0.5in]{geometry}
#+SETUPFILE: https://fniessen.github.io/org-html-themes/org/theme-bigblow.setup

* Data Compression Techniques
Data compression techniques fall into different categories depending on the principle used to develop them.
Most often a combination of different techniques is used to perform compression in real life, since some encoding schemes produce outputs that are not compressed so much, but that can be further compressed efficiently using other techniques.

** Statistical/Entropy Coding Techniques

*** Run Length Coding
- good technique when compressing data that has frequent sequences of repeating characters
- emit a pair (character, length)
- in case there are no repeating characters, results in a "compressed" file that is actually expanded in size because for each 8bit character, we use 8bits + 8bits (say) for the pair
- /*parameters* : repititions in source string (generally no user-settable parameter)/

*** Shannon Fano Coding
- uses probabilities of the characters in the string to assign codes to each character
- the codes are built in a top-down fashion
  - start at the root and keep dividing "nodes"
- the process involves the following:
  1. calculate the probabilities of every character and create a table
  2. sort the table in order of probabilities
  3. divide the table into two such that the resulting tables have approximately the same total probability
  4. assign one table "0" and the other "1"
  5. until all the tables have only one element each, repeating from 'step 3'
- the limitation of this technique is that it does not always produce the optimal sequence
- codeword lengths are within 1 bit of the optimal codes
- have the limitation imposed by integral number of bits per character to encode
- works best in cases where the probabilities of different characters are in powers of two
- decoder requires
  - probability table (to reconstruct the codes in the exact same manner as the encoder)
  - direct mapping between codes and characters in some other form
- /*parameters* : probabilities of occurance of characters (generally no user-settable parameter)/

*** Huffman Coding
- similar in principle to shannon-fano coding
- follows a bottoms-up approach
  - start with leaf nodes and build till convergence is achieved
- the process involves the following
  1. create a list of nodes (one per node) with the probabilities of occurance for that character
  2. pick the two nodes with least probability and make a new node with its probability set to the sum of probabilities of the two nodes
  3. delete the two nodes
  4. goto 'step 2' if there are two or more nodes left
- the constraint of having integral bits per character applies here as well
  - when the probabilities are powers or two, huffman codes give the optimal coding
  - in other cases, the integral bits per character constraint causes characters with probability less than a power of two to have a "fractional bit overhead" and for those with higher probability than a power of two, there is a fractional bit deficit
  - depending on how the probabilities are, the overhead and the deficit result in an overall overhead (unless ofcourse the probabilities are all powers of two)
- this technique also requires the information about probabilities to be handed over to the decode in some form
- one way to attempt at sidestepping the integral number of bits per symbol is to try code for a group of two or more symbols
  - /if the events of the characters in the group are *independent*/ then the probability of the group as a whole is just the product of the individual elements' probabilities
  - problems arise as we try to increase the size of the groups
  - if our message is coded over an alphabet of /m/ letters
    - for a group of 2 elements, we would have $m^2$ entries in the table
    - for a group of 3 elements, we would have $m^3$ entries in the table
    - for a group of /n/ elements, we would have $m^n$ entries in the table
  - it is not practical to evaluate and distribute such large tables as they will end up causing more overhead than the bits saved during compression (also in terms of computation required, this will be very taxing)
- one way to avoid the overhead due to this extra information is to use an adaptive version of the algorithm
  - in the adaptive case, a huffman tree is built on the fly as per the arriving symbol in both the encoder and the decoder
  - each node is attached a "weight" which is the probability and a "number"
  - the "number"s are assigned bottom-to-top and left-to-right
  - it needs to be ensured that if we order the nodes according to weights $$w_1 < w_2 < ... < w_{(2m-1)}$$ then $$n_1 < n_2 < ... < n_{(2m-1)}$$ which besically means that a node with heigher weight must have a higher number
    - /the 2m - 1 comes from the fact that each node in a huffman tree must have a sibling, except for the root/
    - this also means that a parent node must have a higher "number" assigned to it than its children
    - note: /set of nodes that have the same weight/ is called a /block/
  - the UPDATE process is required when a new symbol arrives (at the encoder and decoder):
  - the process of encoding and decoding starts off with a "Not Yet Transmitted" or "NYT" node
  - everytime a new symbol arrives
    - if it is seen for the first time, split the NYT node into two, assign one of the childen the current symbol, and make the other one the new NYT
    - if it has been seen before, visit the symbols node
      - if it has the highest node number in its block, then increment its node number
      - otherwise swap the node with the node with the highest node number in its block and then increment it
    - go to the parent of the currently selected node and repeat till the root node is reached
  - the advantage of building the tree on the fly is that we can avoid the overhead required for packaging the probabilities with the compressed data
    - The algorithm is not free of the integral bits per character constrint though, so we still are not able to exactly reach entropy in most cases
- /*parameters*/ :
  - /probabilities of occurance of characters/
  - /number of characters to clump together for encoding (use-settable)/

*** Arithmetic Coding
- the other techniques mentioned till now all suffer from one common problem
  - they all require an integral number of bits per symbol
- arithmetic coding is a technique that tries to sidestep that problem by changing how strings are encoded
- this scheme tries to encode whole strings rather than encoding single elements
- this technique also requires knowledge of the probabilities of the different symbols in the string
- the procedure is as follows:
  1. start with the interval [0.0, 1.0)
  2. divide the interval according to the probabilities of the characters in the character set
  3. read a character
  4. zoom into the range set by the character just read
  5. repeat 'step 3' till the end of stream character is encountered
  6. once done, choose a number in the region formed that requires the least amount of bits to represent
- this technique basically represents an entire block of the source text as a single real point number, which can be used to obtain the source text by repeating the same zooming process till the end of stream symbol is encountered
- this ofcourse needs infinite precision numbers, which would require an inifinite precision mathematics, which is difficult to perform efficiently
- there are variations and techniques that allow for 16bit, 32bit precicion coding
- we need a solution to the "infinite precision in finite bits" problem
- since we do not have infinite bits to represent out fractions, we end up clipping long fraction that causes deviation from efficiency being the entropy
  - the error can be minimized so that we achieve as close to entropy efficiency as possible
- as the arithmetic coding process continues, the lower and upper bound of the range start to share their upper bits
  - this is a significant observation because it means that if the bounds share some bits, in the next step they will either share the same or more number of bits
  - we can ignore these bits therefore, as once they match, they are inconsequential to further calculations
  - this allows us to use "fixed point" representations for out bounds
  - we simply move out the bits that match ensuring that we can keep going deeper and deeper into as small of ranges as we want

** Burrows Wheeler Transform
- completely different principle when compared to the other techniques described above
- breaks up input into multiple blocks of some fixed size
- uses the /block-sort transform/ to sort the block in a particular manner
- uses the /move-to-front transform/ to convert the sorted contents into a form that is good for other statistical encoders 
- an important observation is that statistical encoders work primarily on the occurance probabilities of characters
  - the probability of occurance does not take into account the "order" or the "context" of occurance of the characters
  - BWT takes the order/context of occurance into account when encoding
- the block-sorting process: (there are some optimizations that can be done, those are not shown for simplicity)
  1. compute all the possible "left cyclic" permutations of the source string
  2. sort the permutations in lexicographical order
  3. extract the last column of the matrix formed by the sorted permutations
  4. extract the position of the original string in the sorted matrix
  5. the last column and the position are the output
- example: source text = BANANA
| permutations | Index | sorted | last column |
|--------------+-------+--------+-------------|
| BANANA       |     1 | ABANAN | N           |
| ANANAB       |     2 | ANABAN | N           |
| NANABA       |     3 | ANANAB | B           |
| ANABAN       |     4 | BANANA | A           |
| NABANA       |     5 | NABANA | A           |
| ABANAN       |     6 | NANABA | A           |
|--------------+-------+--------+-------------|
- output = NNBAAA, 4
- to decode the string, we need only 2 adjecent columns from the sorted table are required
- taking the last column gives us the first column for free (just sorting the last column gives the first column)
- also, since we are rotating towards the left, the first char goes to the end and the second becomes the first... making the last column have the character just left of the first column's character
- so just from the last column we can generate the whole matrix, and then the index that was exported gives the index to the row containing the decoded string
- example : encoded text = NNBAAA, 4
| n | o | s | j  | s  | j   | s   | j    | s    | j     | s     | j      | s      |
| 1 | N | A | NA | AB | NAB | ABA | NABA | ABAN | NABAN | ABANA | NABANA | ABANAN |
| 2 | N | A | NA | AN | NAN | ANA | NANA | ANAB | NANAB | ANABA | NANABA | ANABAN |
| 3 | B | A | BA | AN | BAN | ANA | BANA | ANAN | BANAN | ANANA | BANANA | ANANAB |
| 4 | A | B | AB | BA | ABA | BAN | ABAN | BANA | ABANA | BANAN | ABANAN | *BANANA* |
| 5 | A | N | AN | NA | ANA | NAB | ANAB | NABA | ANABA | NABAN | ANABAN | NABANA |
| 6 | A | N | AN | NA | ANA | NAN | ANAN | NANA | ANANA | NANAB | ANANAB | NANABA |
- /o = output, s = sorted, j = output+previous sorted/
- as can be seen the output is re-formed in the same row pointed to by the index

- the move-to-front process is quite simple as well
  1. prepare the characterset
  2. start from the first character, get its index from the characterset
  3. move the character to the beginning of the characterset
  4. repeat 'step 2' till all the characters are encoded
- the procedure to decode this is also very simple
  1. prepare the characterset
  2. take the character at the index recieved in the encoded string
  3. move the character to the beginning of the characterset
  4. repeat 'step 2' till all the characters are encoded
- why choose the last row?
  - consider an english sentence is to be encoded and it contains many occurances of the word 'has'
  - in all the cyclic rotations of the sentence, many will start with the 'as' from the 'has' and end with the ' of the 'has'
  - when sorted, all these rotations that start with the 'as' are brought together causing the last column to have a cluster of 'h's
  - thus the last row contains clusters of letters which appear in a common pattern
  - hence we choose the last row as the output of the encoder, it has good contextual clustering
- /*parameters*/ :
  - /size of the block of text that we wish to operate on (user-settable)/

** Dictionary Techniques
The Lempel-Ziv algorithms are called dictionary algorithms because they all work in a similar way and require a "dictionary" to be formed while coding and decoding.
The class of algorithms that come under the LZ77 family tree do not construct an explicit dictionary, but instead use the part of the string that has already been decoded/encoded as the dictionary.
On the other hand, the LZ78 family of algorithms maintain a separate explicit dictionary while coding and encoding (the dictionary itself depends only on the part of the string already seen).
These algorithms in general work on the principle that if a pattern has been seen before, it is probably easier to encode the references to the previous occurance for every re-occurance encounteresd.

*** LZ77
- in the LZ77 algorithm, a cursor is maintain which tells the algorithm which character being processed at the time
- all the characters that fall to the left of the cursor (the number of characters to be considered depends on the size set) form the /search buffer/
- the characters that fall on the right of the cursor form the /look ahead buffer/
- the referenced emitted by the encoder are of the form <o, l, c>
  - o is the offset, how far back from the current cursor position the match was found, or 0 if no match was found
  - l is the length of the longest match found, or 0 if no match was found
  - c is the character to the immediate right of the match found, or the character itself if no match was found
- the process followed is as follows:
  1. start at the beginning of the string
  2. look backwards /N/ characters and look for the longest match
  3. if there is no match found emit (0, 0, c) where c is the character under the cursor
  4. else emit (o, l, c) according to the descriptions of o, c, l above
  5. repeat from 'step 2' for every character, skipping l characters if match of that length is found else move by 1
- using values of l larger than the dictionary size allows a run-length type encoding to be made
  - if the pattern length is more than search buffer size, copy the search buffer's characters from left to right and circle back to the start till we finish copying 'length' number of characters
- a bit of a problem with the aggressive coding scheme used is that if there are a lot of unmatched characters, instead of just encoding it as the character itself, the encoding is done as 3 numbers which is a waste of a lot of bits
  - /the schemes that came after this try to fix this problem of over aggressively coding the source (eg: LZSS, snappy)/
- /*parameters*/ :
  - /the size of the search buffer (user-settable)/
    - the larger the search buffer
      - the longer back you can search
      - the more bits you need to represent the offset
      - but you can get references for things that you may not get references for if the search buffer is small
  - /maximum value for longest match (user-settable)/
    - if the longest match length is large we need more bits to represent its value in memory

*** LZ78
- in the LZ78 algorithm too, a cursor is maintain which tells the algorithm which character being processed at the time
- in this algorithm, an explicit dictionary is maintained in memory as the encoding and decoding takes place
- the dictionary is maintained in the form of hashtables or tries
- the dictionary is filled as the cursor moves from the beginning of the source text to the end, using only the characters/patterns visited till that point
  - this property is important, because it allows the decoder to follow the exact same process as the encoder while decoding
  - this property is also why in LZ78 there is no need to distribute the dictionary alongside the encoded text (in huffman coding, a large percentage of overhead is caused due to the fact that the dictionary/mappings need to be shared alongside the encoded string)
- in this technique the encoded stream is of the form <i,c>
  - /i/ is the index of the longest match found in the text to the right of the cursor in the dictionary
  - /c/ is the character immediately following the longest pattern matched (in the text)
- the process of encoding a string is as follows
  1. start from the first character in the source
  2. find the longest match from the source in the dictionary
  3. if no match is found, emit (0, c) where c is the character currently at the cursor
  4. if a match is found, and there is still some text left in the source after the matched text, emit (i, c) where i is the index in the dictionary to the longest match and c is the character in the text immediately following the matched string
  5. if the entire text from the cursor all the way till the end, emit (i,) where i is the index of the match in the dictionary
  6. increment the cursor by the length of the string matched
  7. repeat 'step 2' till all the input is exhausted
- the choice of data structure used to represent the dictionary will have an effect on the speed of finding matches
- /*parameters*/ :
  - /dictionary size (user-settable)/
    - the main parameter of the LZ78 encoder is the size of the dictionary used
    - this is an important parameter since we cannot allow an unbounded growth in dictionary size

*** Some differences between LZ77 and LZ78 schemes
| LZ77                                                                                 | LZ78                                                                    |
|--------------------------------------------------------------------------------------+-------------------------------------------------------------------------|
| registers patterns from their second appearance                                      | registers patters as and when they are first seen                       |
| for every pattern, there is a need to efficiently search the search buffer           | for every pattern simply find its corresponding entry in the dictionary |
| high configurability (search buffer size, maximum length of match, search algorithm) | only configurability comes from the dictionary size                     |
| computation intensive                                                                | memory intensive                                                        |

As CPUs get faster and memory gets cheaper, different ways to mix and match these basic schemes can help in obtaining optimum performance for the system in consideration.
For example dictionary like hashtables are used in gzip to keep 3 character prefixes and the indices of their appearance in the string, even though it(deflate) is based on the LZSS scheme.

*** LZSS
- the LZSS scheme tries and improves upon the LZ77 scheme
- to reduce the overhead caused by aggressive overhead of encoding unmatched single characters as <o,l,c> triplets,
  - if a match is found, emit a <o,l> pair for the offset and length of match
  - if a match is found, but its length is less than 3, then just emit the single characters that form the match as that will take lesser memory to encode than encoding it as a reference
  - otherwise, emit only the single unmatched character
- for the decoder to know whether the next unit is an <o,l> pair or a single character, a ONE BIT flag is used as a prefix - 1 for pairs and 0 for single characters
- this the encoded stream looks like 1(,) 0c 0c 1(,)
- this /requires the capability of reading and writing single bits/ to files which may not be possible. In such cases, a /FLAG byte may be used which encodes in its 8 bits the flags for the next 8 units/
- optimiaztions that can be made in the pattern searching algorithm
  - the deflate scheme that powers the gzip format uses an optimization where it uses a hashing scheme that uses the hashes of 3 character prefix sequences to keep track of matches
  - it stores the index of the patterns occurance in the bucket pointed to by the hash of the first 3 characters of the sequence
  - this makes searching for the pattern simpler as we need to just get the hash, linear search the bucket, and if found report the match index in the hashtable, else find the pattern at the locations mentioned in the hashtable where the 3 character sequence is seen and search the source from there
  - This will be faster than just linear searching the search buffer everytime
- /*parameters*/:
  - the parameters for this algorithm are the same as those for the LZ77 algorithm

*** LZW
- the LZW scheme comes under the LZ78 family of schemes
- instead of storing backreferences using indices, it uses extra bits to make a larger set of symbols available and encodes patterns using these extra symbols
- instead of representing each symbol as an 8bit value (0-255), it represents each symbol as a 9bit value (0-511)
- the dictionary is simply like an array of 512 values, the first 256 of which are initialized with the values of their ascii counterparts
- the process of encoding is as follows:
  1. initialize the first 256 entries of the dictionary/table with their ASCII values
  2. read a character
  3. add the character to the 'working string'
  4. if the working string is present in the dictionary/table
     - *true?*
       - do nothing
     - *false?* /(working string is not present in the table)/
       - let S = substring of working string that excludes only the last character in the working string
       - emit the symbol corresponding to S as the encoding
       - add current working string to the table at the first unused entry
       - set the working string as the last character of the working string
  5. repeat 'step 2' until all the text is encoded
- there is an interesting complication that arises when using this technique (called the cScSc case)
  - condition for the situation to occur:
    - encoder recieves a string of the form cScSc where c is a character, S is a string
    - encoder has already in its dictionary/table the entry for cS
    - encoder has not yet in its dictionary/table the entry for cSc
    - when the encoder sees this pattern
      - sees cSc and emits the symbol corresponding to cS from table
      - adds cSc to the dictionary
      - sees second cSc (from the cSc /cSc/) and encodes it directly as the cSc entry
    - an example of this is $another\_banana$ which is encoded as $another\_b(256)(265)$
      - the 'cScSc' pattern here is 'anana' in 'banana'
      - during decoding, when we reach the point of decoding 265, the table has no entry for it.
      - here we solve the case by following the steps mentioned below
  - the decoder may come across a symbol that is not yet present in its dictionary/table
  - in this case, the next entry in the table is required to be guessed
  - this can be done using the fact that the first character of the working string is going to be the first character in the decoded string
  - this means that in case we need, we can use this assumption and append the first character of the working string to the working string itself to produce the next working string.
    - this can be repeated till we find the entry required, within the dictionary/table
- /*parameters*/:
  - the number of bits used per symbol (generally 9bits)
- this technique requires bit reads and bit writes, which may not be available if the system can operate only on bytes
  - in this case we can examine the values of the 9bits used
  - for the regular ASCII characters, we have values that fit in 8bits
    - this means their 9bit representations look like $0 _ _ _ _ _ _ _ _$
  - for the extra 256 symbols that the 9bit representation offers
    - the representations look like $1 _ _ _ _ _ _ _ _$
  - this means that we can treat the first bit as a FLAG /(0=regular ASCII 1=extended characterset)/
  - this allows systems that can operate only on bits to store the flags of the next 8 symbols in a FLAG BYTE

*** snappy
- the snappy scheme belongs to the LZ77 family of schemes
- it is a byte oriented format (that is, everything is byte aligned, /unlike LZSS/)
- the file starts with an uncompressed length of file, in the form of a little endian /"varint"/
  - a /varint/ is a series of bytes such that in each byte, the lower 7 bits represent some data and the upper bit is set only if there are more bits in the same logical unit ahead
  - used by Google in its [[https://carlmastrangelo.com/blog/lets-make-a-varint][Protocol Buffer]] format.
  - to make a varint, we start off with the number we wish to encode, eg: 330
  - the binary representation of 330 is 0000 0001 0100 1010
  - we need to break it into bytes
    - 0000 0001
    - 0100 1010
  - now, in the lowermost bit, we need to set the uppermost bit to signal that the number is multi-byte
  - so we get 0100 1010 -> 0 1100 1010 (the 0 in the beginning is part of the original number and therefore must be preserved
  - then we add the topmost bit (which came out as a single bit) to the first byte
    - 0000 0001 0
  - which then becomes 0000 0010
  - so when we arrange the bits it now says 0000 0010 1100 1010
  - hence the number 0x014a becomes the varint 0x02ca
  - and then the order must be little endian so it becomes 0xca02
- the rest of the file is a stream of compressed data
- the compressed data contains 2 types of elements
  - literals (characters)
  - back-references
- each element is composed of two parts
  - the tag byte
    - lower 2bits of the tag byte tell the type of data of the coming element
    - 00 for literals
    - 01 for references with 1byte long offset
    - 10 for references with 2byte long offset
    - 11 for references with 4byte long offset
  - the data bytes
- Literals
  - tag byte has lower 2bits as 00
  - the rest (upper 6bits) store the length (len-1) of literal sequence incoming
  - if the 6bits are of values 60/61/62/63 then it indicates that the next 1/2/3/4 bytes encode the actual length
  - the next len bits are uncompressed characters
- Back-references/Copies
  - tag bytes 01, 10, 11 code for offsets of varying byte sizes
  - for 01
    - the upper 6bits of tag byte
      - lower three (bits 3,4,5) store the value of "length-4"
      - upper three (bits 6,7,8) store the upper three bits of 11 bit "offset"
    - the next byte stores the lower 8bits of the "offset"
  - for 10
    - upper 6bits of the tag byte stores the "length-1" value
    - next 2 bytes stores the little endian representation of the "offset"
  - for 11
    - upper 6bits of the tag byte stores the "length-1" value
    - next 4 bytes stores the little endian representation of the "offset"
- Unlike other compression formats like gzip, bzip2 snappy does not perform any kind of entropy coding
